%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Fundamentals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter provides a basic understanding of the underlying methods
and concepts, that were required for this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grids}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In computational science, discrete data domains are often represented by
grids. The data itself is mostly saved at specific points of the grid
(nodes), or in regions (cells), enclosed by surrounding nodes and the
respective connections (edges) between them. In more rare cases the
values are saved in the edges or the faces of the cells. The
connectivity of the nodes is given by the topology of the grid and
therefore the shape of the cells. There are three types of grids:
scattered data, which has no topology, hence no edges connecting the
nodes, structured grids, which have an implicit topology following the
ordering of the nodes, with a fixed number of nodes per dimension, as
well as fixed cell types, and unstructured grids, which only have
irregular topology with varying cell types. For the latter the topology
has to be stored explicitly. Structured grids can further be
distinguished into uniform, rectilinear and curvilinear (irregular)
structured grids. The nodes in uniform grids are equidistant for every
dimension, whereas rectilinear grids may have irregular spacings along
either axis and curvilinear grids may have irregular spacings between
each grid node. This work focuses on uniform structured grids as it
makes it easier to compare multiple grids at specific points in the
domain.\\
INSERT PICTURE TO DIFFERENT GRID TYPES

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional field with a single scalar value at every point in
space is called a scalar field. A simple example for a scalar field is a
height map of some geographical terrain. It has two dimensions with a
height value at every point encoded with color. In this work we will use
the notation $S(x)$ for the scalar value at point $x = (x_1,\dots,x_n)$
in the scalar field $S$ with $S: \real^n \rightarrow \real$. While in
continuous scalar fields the values of every point are defined by a
function, discrete scalar fields only have values at specific points in
space. As real world data is often acquired by measuring certain
locations and usually does not follow any graspable function, we will
focus on discrete scalar fields in this work, where $n \in \{2,3\}$.
This also applies to the other types of fields we will encounter,
because they have the same resolution as the scalar field.\\
INSERT PICTURE OF HEIGHT MAP

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertain Scalar Fields}\label{sec:USF}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Unlike certain scalar fields, uncertain scalar fields have multiple
values for every point of the domain. Following the work of Theisel
\etal{\cite{Vortex}}, we assume that these values are Gaussian
distributed over the $m$ members of the data ensemble obtained from
simulations or measurements and therefore

\begin{equation}
  S_{unc}(x) = \mathcal{N}(\mu_{x}, \sigma_{x}^2),
\end{equation}

\noindent where $\mu_x$ is the mean scalar value and $\sigma_{x}^2$ the
variance of the $m$ entities at point $x = (x_1,\dots,x_n)$. As they
also pointed out in their work, we need to consider the neighborhood of
$x$, so that the derivatives of the uncertain scalar field are Gaussian
distributed as well. As we will explain in Chapter~\ref{chap:Method}, we
will search for ridges (Section~\ref{sec:Ridges}) inside cells.
Therefore, combining this with the need for neighboring points, we have
4 (or 8 in the 3D case) nodes of the cell we want to observe, together
with their neighboring points in every dimension and, because we also
want to calculate the second derivative, their neighboring nodes as
well. Cleared from redundant points, this results in 24 (80) nodes per
neighborhood as illustrated in Figure (insert ref). We can take this
neighborhood as a vector $v_x$ and calculate the $24$-dimensional mean
vector at point $x$ of the mean vector field $m(x) = \frac{1}{m}
\sum_{k=1}^m v_{x,k}$, followed by the $24 \times 24$-dimensional
covariance matrix, with the covariance field being $C(x)= \frac{1}{m}
\sum_{k=1}^m (v_{x,k} - m(x)){(v_{x,k} - m{(x)})}^\top$. Therefore our
uncertain scalar fields follows a multivariate Gaussian distribution
at every location

\begin{equation}
  S_{unc}(x) = \mathcal{N}(m(x), C(x)).\\
\end{equation}

(Picture explaining neighborhood)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vector Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional vector field $V$ associates an $m$-dimensional vector
with every point in space $V(x_1,\dots,x_n):\real^n \rightarrow \real^m$.
If the vector field is the result of deriving a scalar field, thus the 
vector field is the gradient of the scalar field, the vector field is
called conservative. Conservative vector fields have the property, that
the line integral along any path connecting two points is always equal.
Since we are extracting features from scalar fields in this work, our
vector fields will always be conservative and $n = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensor Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional tensor field $H$ associates an $l \times
m$-dimensional tensor with every point in space $H(x_1,\dots,x_n):
\real^n \rightarrow \real^{l \times m}$. Tensor fields are a
generalization of scalar and vector fields as a tensor with $l = m = 1$
would represent a scalar, and with $l > 1$ and $m = 1$ a vector. In our
case, the tensor field is a result of deriving a vector field, thus $n =
l = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since we are dealing with discrete scalar fields, we have no function we
could derive to get the underlying gradient field. Instead we have to
approximate the derivatives with finite difference methods. There are
three forms which are commonly used:\\
\\
\begin{inparaenum}[(a)]
  \item Forward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_i)}{h}
  \end{equation}
  \item Backward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_i) - f(x_{i-1})}{h}
  \end{equation}
  \item Central Differences
  \begin{equation}\label{eq:centDiff}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_{i-1})}{2h}
  \end{equation}
\end{inparaenum}
where $f(x_i)$ denotes the $i$-th point of the function $f(x): \real
\rightarrow \real$ and $h$ the distance between two neighboring points.
Forward and backward differences are used at the borders of the field,
where no previous or succeeding point is available. As we will explain
in Chapter~\ref{chap:Method}, with our current implementation we process
cells with a fixed size and therefore do not consider border cases, thus
this work focuses on central differences.

%----------------------------------------------------------------------%
\subsection{First Derivative}
%----------------------------------------------------------------------%

When deriving an $n$-dimensional scalar field $S$, we need to apply
Equation~\ref{eq:centDiff} for every dimension. This results in $n$
scalar values, representing the change of the scalar field in the
respective dimensions. These values can be interpreted as an
$n$-dimensional vector, pointing in the direction of greatest ascend for
every point of the scalar field. This is the gradient field $\nabla
S(x)$.

%----------------------------------------------------------------------%
\subsection{Second Derivative}
%----------------------------------------------------------------------%

The second derivative of an $n$-dimensional scalar field is an $n$
-dimensional tensor field with $\nabla^2 S:\real^n \rightarrow \real^{n
\times n}$. This tensor field can again be obtained with the central
difference method applied to every dimension of the gradient field. Here
we get an $n$-dimensional vector per dimension as we are subtracting the
previous gradient from the succeeding gradient of point $x$ along any
dimension. These vectors represent the respective columns of the so
called Hessian matrix. The Hessian matrix $H$ is a specification of the
Jacobian Matrix $J$, which derives any vector valued function $f(x):
\real^n \rightarrow \real^m$, $J \in \real^{m \times n}$. The Hessian
matrix is obtained when deriving a conservative vector field, therefore
the Hessian is quadratic with:

\begin{equation}
  H =
  \begin{bmatrix}
    \frac{\partial^2 S}{\partial x_1^2} & \frac{\partial^2 S}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_1 \partial x_n}\\
    \frac{\partial^2 S}{\partial x_2 \partial x_1} & \frac{\partial^2 S}{\partial x_2^2} & \dots & \frac{\partial^2 S}{\partial x_2 \partial x_n}\\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 S}{\partial x_n \partial x_1} & \frac{\partial^2 S}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_n^2}
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}

\noindent According to Schwarz's theorem of the symmetry of second
derivatives ([ref]), the Hessian is assumed to be symmetric, therefore
the eigenvectors are orthogonal and the eigenvalues are real valued.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Interpolation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our discrete fields only provide information for a limited number of
points along a dimension, therefore we need to estimate the values lying
between the known locations. To ease computation we assume that the
underlying functions of the fields are linear and apply linear
interpolation to the tensors $A, B \in \real^{n \times n}$ at the
neighboring points, independent of dimensionality. As we will explain in
Chapter~\ref{chap:Method}, we only apply interpolation on the edges of
cells and can therefore assume the distance between the two nodes to be
$1$. This leads to the interpolated tensor:

\begin{equation}
  T_{ipol} =
  \begin{bmatrix}
    (1-t) \cdot A_{11} + t \cdot B_{11} & \dots & (1-t) \cdot A_{1n} + t \cdot B_{1n} \\
    \vdots & \ddots & \vdots \\
    (1-t) \cdot A_{n1} + t \cdot B_{n1} & \dots & (1-t) \cdot A_{nn} + t \cdot B_{nn} \\
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}
with $t$ being the relative distance of the desired location to tensor
$A$. (see Figure ipol) We will only consider the cases for $n \in
\{1,2,3\}$. \\
(Picture explaining linear interpolation)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Decompositions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A matrix decomposition is a factorization of a matrix into its
constituent parts. In general there are two classes of factorizations:
decompositions used for solving linear equations and decompositions
based on eigenvalues and vectors. We will consider one example from each
class, namely the Cholesky and the Eigendecomposition. As we will
explain in Section~\ref{sec:MGS} we will use both decompositions to
generate samples from a multivariate gaussian distribution, even though
they are from different classes.

%----------------------------------------------------------------------%
\subsection{Cholesky Decomposition}
%----------------------------------------------------------------------%

The Cholesky factorization is a decomposition of a square, hermititan,
positive definite matrix $A$ into the product of a lower-triangular
matrix L and its conjugate transpose $A = L L^*$. A matrix $C$ is
hermitian if the diagonal elements are real valued and $C_{ij} =
\overline{C_{ji}}$, meaning that $\overline{C_{ji}}$ is the complex
conjugate of $C_{ij}$. Two complex numbers are complex conjugate if they
have equal real and imaginary parts, but opposite signs. For example,
$1+i$ is the complex conjugate of $1-i$. Following this property, the
standard Cholesky decomposition is also applicable for every real
valued, symmetric, positive definite matrix, resulting in a lower
triangular real valued matrix and its transpose $A = L L^\top$. We
will only deal with real valued matrices in this work and therefore
follow this notation. A simple way to determine the definiteness of a
matrix is to look at its eigenvalues. If every eigenvalue $\lambda$ of a
square $n \times n$ matrix is greater than zero, $\lambda_{i} > 0, i \in
\{1,\dots,n\}$, the matrix is positive definite. If every eigenvalue is
greater or equal to zero, $\lambda_{i} \ge 0$, the matrix is positive
semi-definite. The same goes for negative eigenvalues, as they make the
matrix negative (semi-) definite.

%----------------------------------------------------------------------%
\subsubsection{$LDL^T$ Decomposition}
%----------------------------------------------------------------------%

The closely related $LDL^T$ decomposition extends the classical Cholesky
decomposition to semi-definite and negative definite matrices $A = L D
L^\top$, where $L$ is a lower unitriangular matrix, meaning there are
only ones on the diagonal, and $D$ is a diagonal matrix. The two
decompositions are related as follows:

\begin{equation}
  A = LDL^\top = (L D^\frac{1}{2}) (D^\frac{1}{2}  L^\top)
\end{equation}

\noindent This version has the advantage that it avoids extracting
square roots, thus allowing for negative entries in $D$, as it is
possible for some indefinite matrices for which no classical
decomposition exists, while having the same computational complexity as
the Cholesky decomposition. If the solution of a linear system is needed
and the system can be put into a symmetric form, the Cholesky
decomposition and its variant is the method of choice, as it offers
efficiency and usually numerical stability. Here is an example for both
variants of the decomposition:\\
\begin{inparaenum}[]
  \item Cholesky decomposition of a symmetric real valued matrix
  \begin{equation}
    \begin{pmatrix}
      9 & 18 & -27\\
      18 & 40 & -18 \\
      -27 & -18 & 406
    \end{pmatrix}
    =
    \begin{pmatrix}
      3 & 0 & 0\\
      6 & 2 & 0 \\
      -9 & 18 & 1
    \end{pmatrix}
    \begin{pmatrix}
      3 & 6 & -9\\
      0 & 2 & 18 \\
      0 & 0 & 1
    \end{pmatrix}
  \end{equation}
  \item $LDL^T$ decomposition of the same matrix
  \begin{equation}
    \begin{pmatrix}
      9 & 18 & -27\\
      18 & 40 & -18 \\
      -27 & -18 & 406
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & 0 & 0\\
      2 & 1 & 0 \\
      -3 & 9 & 1
    \end{pmatrix}
    \begin{pmatrix}
      9 & 0 & 0\\
      0 & 4 & 0 \\
      0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 2 & -3\\
      0 & 1 & 9 \\
      0 & 0 & 1
    \end{pmatrix}
  \end{equation}
\end{inparaenum}

%----------------------------------------------------------------------%
\subsection{Eigendecomposition}
%----------------------------------------------------------------------%

The Eigendecomposition is a factorization of a square $n \times n$,
diagonizable matrix $A$, into the form $A = E \Lambda E^{-1}$, where the
columns of $E$ are $n$ linearly independent eigenvectors of $A$ and
$\Lambda$ is a diagonal matrix with the respective eigenvalues. The
eigenvalue $\lambda_i$ at $\Lambda_{ii}$ corresponds to the eigenvector
$\epsilon_i$ at column $i, i \in \{1,\dots,n\}$ of $E$. This follows the
basic attribute of eigenvectors, as they only change by a scalar factor
when multiplied with their respective matrix, meaning $A \epsilon =
\lambda \epsilon$ and therefore $A E = E \Lambda$, which leads to the
decomposition $A = E \Lambda E^{-1}$. Heres a simple example of the
Eigendecomposition of a symmetric, real, diagonizable matrix:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & {-1} & 1\\
    1 & 0 & 1\\
    2 & 1 & 0
  \end{pmatrix}
  \begin{pmatrix}
    4 & 0 & 0\\
    0 & {-2} & 0\\
    0 & 0 & {-2}
  \end{pmatrix}
  \begin{pmatrix}
    0.5 & {-0.5} & 0.5\\
    {-1} & 1 & 0\\
    {-0.5} & 1.5 & {-0.5}
  \end{pmatrix}
\end{equation}

An $n \times n$-dimensional matrix is diagonizable if it has $n$
linearly independent eigenvectors. The decomposition can for example be
used to easily raise a matrix to a power, as you only have to raise the
eigenvalue matrix $\Lambda$ to that power:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  ^4
  =
  E
  \Lambda^4
  E^{-1}
  =
  \begin{pmatrix}
    136 & {-120} & 120\\
    120 & {-104} & 120\\
    240 & {-240} & 256
  \end{pmatrix}
\end{equation}

In this work, our matrices don't need to be diagonizable, as this
property is only required for the inverse of the eigenvector matrix.
Section~\ref{sec:MGS} will give more information on that matter. 

%----------------------------------------------------------------------%
\subsubsection{Principal Component Analysis}
%----------------------------------------------------------------------%

Even though the Principal Component Analysis (PCA) is not a decomposition
in itself, it is an application of the Eigendecomposition. In general,
the PCA fits an $n$-dimensional ellipsoid around a set of possibly
correlated points. The first principle component then corresponds to the
axis of biggest magnitude of the ellipsoid and therefore the axis of the
largest variance of the data. The principal components are always
orthogonal to each other. When the Eigendecomposition is applied to
a matrix containing correlated points, the resulting eigenvectors match
the principal components, and the eigenvalues the variance along those
axes.
\\(PCA PICTURE)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Box-Muller Transformation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Box-Muller Transformation is a method to transform two uniformly
distributed random numbers into two independent, standard, normally
distributed random numbers. Let $U_1$ and $U_2$ be samples from the
uniform distribution on the interval ($0$, $1$). Let $N_1$ and $N_2$ be:

\begin{align}
  N_1 = \sqrt{-2 \ln{U_1}} \cos (2 \pi U_2) \\
  N_2 = \sqrt{-2 \ln{U_1}} \sin (2 \pi U_2)
\end{align}

\noindent Then $N_1$ and $N_2$ are independent random numbers with a
standard normal distribution, meaning that their mean is $\mu = 0$ and
standard deviation $\sigma = 1$. This transformation can be repeated to
generate standard normal distributed vectors of arbitrary length.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridges}\label{sec:Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Marching Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Vectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
