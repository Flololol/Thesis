%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Fundamentals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter provides a basic understanding of the underlying methods
and concepts, that were required for this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grids}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In computational science, discrete data domains are often represented by
grids. The data itself is mostly saved at specific points of the grid
(nodes), or in regions (cells), enclosed by surrounding nodes and the
respective connections (edges) between them. In more rare cases the
values are saved in the edges or the faces of the cells. The
connectivity of the nodes is given by the topology of the grid and
therefore the shape of the cells. There are three types of grids:
scattered data, which has no topology, hence no edges connecting the
nodes, structured grids, which have an implicit topology following the
ordering of the nodes, with a fixed number of nodes per dimension, as
well as fixed cell types, and unstructured grids, which only have
irregular topology with varying cell types. For the latter the topology
has to be stored explicitly. Structured grids can further be
distinguished into uniform, rectilinear and curvilinear (irregular)
structured grids. The nodes in uniform grids are equidistant for every
dimension, whereas rectilinear grids may have irregular spacings along
either axis and curvilinear grids may have irregular spacings between
each grid node. This work focuses on uniform structured grids as it
makes it easier to compare multiple grids at specific points in the
domain.\\
INSERT PICTURE TO DIFFERENT GRID TYPES

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional field with a single scalar value at every point in
space is called a scalar field. A simple example for a scalar field is a
height map of some geographical terrain. It has two dimensions with a
height value at every point encoded with color. In this work we will use
the notation $S(x)$ for the scalar value at point $x = (x_1,\dots,x_n)$
in the scalar field $S$ with $S: \real^n \rightarrow \real$. While in
continuous scalar fields the values of every point are defined by a
function, discrete scalar fields only have values at specific points in
space. As real world data is often acquired by measuring certain
locations and usually does not follow any graspable function, we will
focus on discrete scalar fields in this work. This also applies to the
other types of fields we will encounter, because they have the same 
resolution as the scalar field.\\
INSERT PICTURE OF HEIGHT MAP

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertain Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Notation of uncertainty. Read Paper!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vector Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional vector field $V$ associates an $m$-dimensional vector
with every point in space $V(x_1,\dots,x_n):\real^n \rightarrow \real^m$.
If the vector field is the result of deriving a scalar field, thus the 
vector field is the gradient of the scalar field, the vector field is
called conservative. Conservative vector fields have the property, that
the line integral along any path connecting two points is always equal.
Since we are extracting features from scalar fields in this work, our
vector fields will always be conservative and $n = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensor Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional tensor field $H$ associates an $l \times
m$-dimensional tensor with every point in space $H(x_1,\dots,x_n):
\real^n \rightarrow \real^{l \times m}$. Tensor fields are a
generalization of scalar and vector fields as a tensor with $l = m = 1$
would represent a scalar, and with $l > 1$ and $m = 1$ a vector. In our
case, the tensor field is a result of deriving a vector field, thus $n =
l = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since we are dealing with discrete scalar fields, we have no function we
could derive to get the underlying gradient field. Instead we have to
approximate the derivatives with finite difference methods. There are
three forms which are commonly used:\\
\\
\begin{inparaenum}[(a)]
  \item Forward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_i)}{h}
  \end{equation}
  \item Backward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_i) - f(x_{i-1})}{h}
  \end{equation}
  \item Central Differences
  \begin{equation}\label{eq:centDiff}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_{i-1})}{2h}
  \end{equation}
\end{inparaenum}
where $f(x_i)$ denotes the $i$-th point of the function $f(x): \real
\rightarrow \real$ and $h$ the distance between two neighboring points.
Forward and backward differences are used at the borders of the field,
where no previous or succeeding point is available. As we will explain
in Chapter~\ref{chap:Method}, with our current implementation we process
cells with a fixed size and therefore do not consider border cases, thus
this work focuses on central differences.

%----------------------------------------------------------------------%
\subsection{First Derivative}
%----------------------------------------------------------------------%

When deriving an $n$-dimensional scalar field $S$, we need to apply
Equation~\ref{eq:centDiff} for every dimension. This results in $n$
scalar values, representing the change of the scalar field in the
respective dimensions. These values can be interpreted as an
$n$-dimensional vector, pointing in the direction of greatest ascend for
every point of the scalar field. This is the gradient field $\nabla
S(x)$.

%----------------------------------------------------------------------%
\subsection{Second Derivative}
%----------------------------------------------------------------------%

The second derivative of an $n$-dimensional scalar field is an $n$
-dimensional tensor field with $\nabla^2 S:\real^n \rightarrow \real^{n
\times n}$. This tensor field can again be obtained with the central
difference method applied to every dimension of the gradient field. Here
we get an $n$-dimensional vector per dimension as we are subtracting the
previous gradient from the succeeding gradient of point $x$ along any
dimension. These vectors represent the respective columns of the so
called Hessian matrix. The Hessian matrix $H$ is a specification of the
Jacobian Matrix $J$, which derives any vector valued function $f(x):
\real^n \rightarrow \real^m$, $J \in \real^{m \times n}$. The Hessian
matrix is obtained when deriving a conservative vector field, therefore
the Hessian is quadratic with:

\begin{equation}
  H =
  \begin{bmatrix}
    \frac{\partial^2 S}{\partial x_1^2} & \frac{\partial^2 S}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_1 \partial x_n}\\
    \frac{\partial^2 S}{\partial x_2 \partial x_1} & \frac{\partial^2 S}{\partial x_2^2} & \dots & \frac{\partial^2 S}{\partial x_2 \partial x_n}\\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 S}{\partial x_n \partial x_1} & \frac{\partial^2 S}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_n^2}
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}

\noindent According to Schwarz's theorem of the symmetry of second
derivatives ([ref]), the Hessian is assumed to be symmetric, therefore
the eigenvectors are orthogonal and the eigenvalues are real valued.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Interpolation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our discrete fields only provide information for a limited number of
points along a dimension, therefore we need to estimate the values lying
between the known locations. To ease computation we assume that the
underlying functions of the fields are linear and apply linear
interpolation to the tensors $A, B \in \real^{n \times n}$ at the
neighboring points, independent of dimensionality. As we will explain in
Chapter~\ref{chap:Method}, we only apply interpolation on the edges of
cells and can therefore assume the distance between the two nodes to be
$1$. This leads to the interpolated tensor:

\begin{equation}
  T_{ipol} =
  \begin{bmatrix}
    (1-t) \cdot A_{11} + t \cdot B_{11} & \dots & (1-t) \cdot A_{1n} + t \cdot B_{1n} \\
    \vdots & \ddots & \vdots \\
    (1-t) \cdot A_{n1} + t \cdot B_{n1} & \dots & (1-t) \cdot A_{nn} + t \cdot B_{nn} \\
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}
with $t$ being the relative distance of the desired location to tensor
$A$. (see Figure ipol) We will only consider the cases for $n \in
\{1,2,3\}$. \\
(Picture explaining linear interpolation)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Decompositions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A matrix decomposition is a factorization of a matrix into its
constituent parts. In general there are two classes of factorizations:
decompositions used for solving linear equations and decompositions
based on eigenvalues and vectors. We will consider one example from each
class, namely the Cholesky and the Eigendecomposition. As we will
explain in Section~\ref{sec:MGS} we will use both decompositions to
generate samples from a multivariate gaussian distribution, even though
they are from different classes.

%----------------------------------------------------------------------%
\subsection{Cholesky Decomposition}
%----------------------------------------------------------------------%

The Cholesky factorization is a decomposition of a square, hermititan,
positive definite matrix $A$ into the product of a lower-triangular
matrix L and its conjugate transpose $A = L L^*$. A matrix $C$ is
hermitian if the diagonal elements are real valued and $C_{ij} =
\overline{C_{ji}}$, meaning that $\overline{C_{ji}}$ is the complex
conjugate of $C_{ij}$. Two complex numbers are complex conjugate if they
have equal real and imaginary parts, but opposite signs. For example,
$1+i$ is the complex conjugate of $1-i$. Following this property, the
standard Cholesky decomposition is also applicable for every real
valued, symmetric, positive definite matrix, resulting in a lower
triangular real valued matrix and its transpose $A = L L^\top$. We
will only deal with real valued matrices in this work and therefore
follow this notation. A simple way to determine the definiteness of a
matrix is to look at its eigenvalues. If every eigenvalue $\lambda$ of a
square $n \times n$ matrix is greater than zero, $\lambda_{i} > 0, i \in
\{1,\dots,n\}$, the matrix is positive definite. If every eigenvalue is
greater or equal to zero, $\lambda_{i} \ge 0$, the matrix is positive
semi-definite. The same goes for negative eigenvalues, as they make the
matrix negative (semi-) definite.

%----------------------------------------------------------------------%
\subsubsection{$LDL^T$ Decomposition}
%----------------------------------------------------------------------%

The closely related $LDL^T$ decomposition extends the classical Cholesky
decomposition to semi-definite and negative definite matrices $A = L D
L^\top$, where $L$ is a lower unitriangular matrix, meaning there are
only ones on the diagonal, and $D$ is a diagonal matrix. The two
decompositions are related as follows:

\begin{equation}
  A = LDL^\top = (L D^\frac{1}{2}) (D^\frac{1}{2}  L^\top)
\end{equation}

\noindent This version has the advantage that it avoids extracting
square roots, thus allowing for negative entries in $D$, as it is
possible for some indefinite matrices for which no classical
decomposition exists, while having the same computational complexity as
the Cholesky decomposition. If the solution of a linear system is needed
and the system can be put into a symmetric form, the Cholesky
decomposition and its variant is the method of choice, as it offers
efficiency and usually numerical stability. Here is an example for both
variants of the decomposition:\\
\begin{inparaenum}[]
  \item Cholesky decomposition of a symmetric real valued matrix
  \begin{equation}
    \begin{pmatrix}
      9 & 18 & -27\\
      18 & 40 & -18 \\
      -27 & -18 & 406
    \end{pmatrix}
    =
    \begin{pmatrix}
      3 & 0 & 0\\
      6 & 2 & 0 \\
      -9 & 18 & 1
    \end{pmatrix}
    \begin{pmatrix}
      3 & 6 & -9\\
      0 & 2 & 18 \\
      0 & 0 & 1
    \end{pmatrix}
  \end{equation}
  \item $LDL^T$ decomposition of the same matrix
  \begin{equation}
    \begin{pmatrix}
      9 & 18 & -27\\
      18 & 40 & -18 \\
      -27 & -18 & 406
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & 0 & 0\\
      2 & 1 & 0 \\
      -3 & 9 & 1
    \end{pmatrix}
    \begin{pmatrix}
      9 & 0 & 0\\
      0 & 4 & 0 \\
      0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 2 & -3\\
      0 & 1 & 9 \\
      0 & 0 & 1
    \end{pmatrix}
  \end{equation}
\end{inparaenum}

%----------------------------------------------------------------------%
\subsection{Eigendecomposition}
%----------------------------------------------------------------------%

The Eigendecomposition is a factorization of a square $n \times n$,
diagonizable matrix $A$, into the form $A = E \Lambda E^{-1}$, where the
columns of $E$ are $n$ linearly independent eigenvectors of $A$ and
$\Lambda$ is a diagonal matrix with the respective eigenvalues. The
eigenvalue $\lambda_i$ at $\Lambda_{ii}$ corresponds to the eigenvector
$\epsilon_i$ at column $i, i \in \{1,\dots,n\}$ of $E$. This follows the
basic attribute of eigenvectors, as they only change by a scalar factor
when multiplied with their respective matrix, meaning $A \epsilon =
\lambda \epsilon$ and therefore $A E = E \Lambda$, which leads to the
decomposition $A = E \Lambda E^{-1}$. Heres a simple example of the
Eigendecomposition of a symmetric, real, diagonizable matrix:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & {-1} & 1\\
    1 & 0 & 1\\
    2 & 1 & 0
  \end{pmatrix}
  \begin{pmatrix}
    4 & 0 & 0\\
    0 & {-2} & 0\\
    0 & 0 & {-2}
  \end{pmatrix}
  \begin{pmatrix}
    0.5 & {-0.5} & 0.5\\
    {-1} & 1 & 0\\
    {-0.5} & 1.5 & {-0.5}
  \end{pmatrix}
\end{equation}

An $n \times n$-dimensional matrix is diagonizable if it has $n$
linearly independent eigenvectors. The decomposition can for example be
used to easily raise a matrix to a power, as you only have to raise the
eigenvalue matrix $\Lambda$ to that power:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  ^4
  =
  E
  \Lambda^4
  E^{-1}
  =
  \begin{pmatrix}
    136 & {-120} & 120\\
    120 & {-104} & 120\\
    240 & {-240} & 256
  \end{pmatrix}
\end{equation}

In this work, our matrices don't need to be diagonizable, as this
property is only required for the inverse of the eigenvector matrix.
Section~\ref{sec:MGS} will give more information on that matter. 

%----------------------------------------------------------------------%
\subsubsection{Principal Component Analysis}
%----------------------------------------------------------------------%

Even though the Principal Component Analysis (PCA) is not a decomposition
in itself, it is an application of the Eigendecomposition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multivariate Gaussian Sampling}\label{sec:MGS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Marching Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Vectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
