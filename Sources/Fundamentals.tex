%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Fundamentals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter provides a basic understanding of the underlying methods
and concepts, that were required for this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grids}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In computational science, discrete data domains are often represented by
grids. The data itself is mostly saved at specific points of the grid
(nodes), or in regions (cells), enclosed by surrounding nodes and the
respective connections (edges) between them. In more rare cases the
values are saved in the edges or the faces of the cells. The
connectivity of the nodes is given by the topology of the grid and
therefore the shape of the cells. There are three types of grids:
scattered data (Figure~\ref{fig:scattered}), which has no topology,
hence no edges connecting the nodes, structured grids, which have an
implicit topology following the ordering of the nodes, with a fixed
number of nodes per dimension, as well as fixed cell types, and
unstructured grids (Figure~\ref{fig:unstructured}), which only have
irregular topology with varying cell types. For the latter the topology
has to be stored explicitly. Structured grids can further be
distinguished into uniform (Figure~\ref{fig:uniform}), rectilinear
(Figure~\ref{fig:rectilinear}) and curvilinear structured grids
(Figure~\ref{fig:curvilinear}). The nodes in uniform grids are
equidistant for every dimension, whereas rectilinear grids may have
irregular spacings along either axis and curvilinear grids may have
irregular spacings between each grid node. This work focuses on uniform
structured grids as it makes it easier to compare multiple grids at
specific points in the domain. Due to the structured grids, we have a
fixed number of nodes along a dimension, which determine the resolution
of this axis.

\begin{figure}
  \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{Images/scattered.pdf}
    \caption{Scattered}
    \label{fig:scattered}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\textwidth}
    \includegraphics[width=\textwidth]{Images/unstructured.pdf}
    \caption{Unstructured}
    \label{fig:unstructured}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\textwidth}
    \includegraphics[width=\textwidth]{Images/uniform.pdf}
    \caption{Uniform}
    \label{fig:uniform}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\textwidth}
    \includegraphics[width=\textwidth]{Images/rectilinear.pdf}
    \caption{Rectilinear}
    \label{fig:rectilinear}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{Images/curvilinear.pdf}
    \caption{Curvilinear}
    \label{fig:curvilinear}
  \end{subfigure}
  \caption{Different types of grids. Uniform structured grids like in
  Figure~\ref{fig:uniform} can have varying distances inbetween
  dimensions, but along a dimension, the distances of the nodes are
  equal. The Figures~\subref{fig:uniform}, \subref{fig:rectilinear} and
  \subref{fig:curvilinear} are variants of structured grids and
  therefore their connectivity is given by the ordering of the nodes.}
  \label{fig:grids}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth]{Images/microfluid.png}
  \caption{3-dimensional view of a color coded 2-dimensional heightmap.
  The color scale to the right gives information on the height values in
  the visualization. This image shows a part of a microfluidic channel,
  observed with a microscope. Microfluidics deal with the manipulation
  and controlling of fluids in the range of micro- to pictoliters.
  Picture from ZEISS Microscopy~\cite{HM}.}
  \label{fig:HM}
\end{figure}

An $n$-dimensional field with a single scalar value at every point in
space is called a scalar field. A simple example for a scalar field is a
height map of some geographical terrain. It has two dimensions with a
height value at every point, usually encoded with color (Figure
\ref{fig:HM}). In this work we will use the notation $S(\boldsymbol{x})$
for the scalar value at point $\boldsymbol{x} = (x_1,\dots,x_n)$ in the
scalar field $S$ with $S: \real^n \rightarrow \real$. While in
continuous scalar fields the value of every point is defined by a
function, discrete scalar fields only have values at specific points in
space. As real world data is often acquired by measuring certain
locations and usually does not follow any graspable function, we will
focus on uniformly structured discrete scalar fields in this work, where
$n \in \{2,3\}$. This also applies to the other types of fields we will
encounter, because they have the same resolution as the scalar field.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vector Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional vector field $V$ associates an $m$-dimensional vector
with every point in space $V(x_1,\dots,x_n):\real^n \rightarrow \real^m$.
If the vector field is the result of deriving a scalar field, thus the 
vector field is the gradient of the scalar field, the vector field is
called conservative. Conservative vector fields have the property, that
the line integral along any path connecting two points is always equal.
Since we are extracting features from scalar fields in this work, our
vector fields will always be conservative and $n = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensor Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional tensor field $H$ associates an arbitrarily
dimensional tensor with every point in space $H(x_1,\dots,x_n): \real^n
\rightarrow \real^{m_1 \times \cdots \times m_k}$, where $k \in
\mathbb{N}$ is the rank of the tensor. Tensor fields are a
generalization of scalar and vector fields as a tensor of rank 0 would
represent a scalar and a tensor of rank 1 a vector. In our case, besides
scalars and vector, the tensors we encounter are of rank 2 and therefore
matrices in $\real^ {m \times l}$. As we will see later, the matrices
are covariance and Hessian matrices and therefore quadratic per
definition with $m = l$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Interpolation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{Images/linipol.pdf}
  \caption{Tensors $A$ and $B$ at nodes in a discrete field. Linear
  interpolation can be thought of as a percentage mixing of two tensors.
  Therefore if the Tensor $T$ we want to interpolate is for
  example at $20\%$ of the distance between $A$ and $B$, we add $80\%$
  of $A$ to $20\%$ of $B$, as $T$ is more influenced by $A$,
  because of its proximity.}
  \label{fig:ipol}
\end{figure}

Our discrete fields only provide information for a limited number of
points along a dimension, therefore we need to estimate values lying
between known locations. To ease computation we assume that the
underlying functions of the fields are linear, allowing us to use simple
linear interpolation. As we will explain in Chapter~\ref{chap:Method},
we will only interpolate between neighboring nodes of a cell and can
therefore assume their distance to be 1. As we can express two
neighboring scalars, vectors or matrices as tensors of rank 2 with $A, B
\in \real^{n \times m}$ and $n,m \in \{1,2,3\}$, we generalize the
interpolation to:
\begin{equation}
  T =
  \begin{bmatrix}
    (1-t) \cdot A_{11} + t \cdot B_{11} & \dots & (1-t) \cdot A_{n1} + t \cdot B_{n1} \\
    \vdots & \ddots & \vdots \\
    (1-t) \cdot A_{1m} + t \cdot B_{1m} & \dots & (1-t) \cdot A_{nm} + t \cdot B_{nm} \\
  \end{bmatrix}
  \in \real^{n \times m},
\end{equation}
with $T$ being the interpolated tensor at the relative distance $t$ to
tensor $B$ (see Figure~\ref{fig:ipol}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Derivatives}\label{sec:derivatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since we are dealing with discrete scalar fields, we have no function we
could derive to get the underlying gradient field. Instead we have to
approximate the derivatives with finite difference methods. There are
three forms which are commonly used:\\
\begin{inparaenum}[(a)]
  \item Forward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_i)}{h},
  \end{equation}
  \item Backward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_i) - f(x_{i-1})}{h},
  \end{equation}
  \item Central Differences
  \begin{equation}\label{eq:centDiff}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_{i-1})}{2h},
  \end{equation}
\end{inparaenum}
where $f(x_i)$ denotes the $i$-th point of the function $f(x): \real
\rightarrow \real$ and $h$ the distance between two neighboring points.
This gives us the change of $f(x_i)$ along its only dimension. If we are
dealing with higher order functions, we need to reapply the difference
methods to all dimensions of the function. Forward and backward
differences are only used to estimate the gradient at the borders of
fields, where no previous or succeeding point is available. They yield
lower precision for the estimation, because they are missing information
about the change in one direction along the dimension. As we will
explain in Chapter~\ref{chap:Method}, with our current implementation we
process cells with a fixed size to always have the higher precision of
central differences. We therefore do not consider border cases and have
a padding of unprocessed cells around our results.

%----------------------------------------------------------------------%
\subsection{First Derivative}
%----------------------------------------------------------------------%

When deriving an $n$-dimensional scalar field $S$, we need to apply
Equation~\ref{eq:centDiff} for every dimension. This results in $n$
scalar values, representing the change of the scalar field in the
respective dimension. These values can be interpreted as an
$n$-dimensional vector, the gradient, pointing in the direction of
greatest ascend for every point of the scalar field. This is the
vector field $\nabla S(x)$.

%----------------------------------------------------------------------%
\subsection{Second Derivative}
%----------------------------------------------------------------------%

The second derivative of an $n$-dimensional scalar field is an $n$
-dimensional tensor field with $\nabla^2 S:\real^n \rightarrow \real^{n
\times n}$. This tensor field can again be obtained with the central
difference method applied to every dimension of the gradient field. Here
we get an $n$-dimensional vector per dimension as we are subtracting the
previous gradient from the succeeding gradient of point $x$ along any
dimension. These vectors represent the respective columns of the so
called Hessian matrix. The Hessian matrix $H$ is a specification of the
Jacobian Matrix $J$, which derives any vector valued function $f(x):
\real^n \rightarrow \real^m$, $J \in \real^{m \times n}$. The Hessian
denotes the second derivative of a scalar field with

\begin{equation}
  H =
  \begin{bmatrix}
    \frac{\partial^2 S}{\partial x_1^2} & \frac{\partial^2 S}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_1 \partial x_n}\\
    \frac{\partial^2 S}{\partial x_2 \partial x_1} & \frac{\partial^2 S}{\partial x_2^2} & \dots & \frac{\partial^2 S}{\partial x_2 \partial x_n}\\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 S}{\partial x_n \partial x_1} & \frac{\partial^2 S}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_n^2}
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}

\noindent and some resulting particularities. According to Schwarz's
theorem of the symmetry of second derivatives~\cite{Schwarz}, the
Hessian is assumed to be symmetric, therefore the eigenvectors are
orthogonal and the eigenvalues are real valued. The eigenvalues of the
Hessian denote the second directional derivative of the scalar field
along the direction of the corresponding eigenvector, and thus the
change of the gradient in the direction of the eigenvector. The
eigenvectors are the axis along which the gradient changes the most and
the eigenvalues determine by how much. Independent of the direction
taken along the eigenvector, a negative eigenvalue pushes the gradient
against the direction taken, whereas a positive eigenvalue scales the
gradient along its direction. Figure~\ref{fig:gradEV} explains this
relation on a simple example.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{Images/gradEV.pdf}
  \caption{Influence of the eigenvectors and the sign of their
  eigenvalues on the gradient (green) along their direction. The
  eigenvalues show us, that if we take a step along the direction of the
  gradient, the gradient turns into the direction depicted by the black
  arrow, as he is scaled along the red eigenvector and squished along
  the blue eigenvector.}
  \label{fig:gradEV}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Uncertain Scalar Fields}\label{sec:USF}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{Images/gauss.pdf}
  \caption{Probability density function for a random variable $X$ from a
   Gaussian distribution for different values of the mean and variance.
   Low variance (blue) equals a high probability for values close to the
   mean, whereas high variance (yellow) leads to wider spread
   probabilities. The mean value determines the peak of the probability
   curve. The red curve shows a standard normal distribution with mean
   $\mu = 0$ and variance $\sigma^2 = 1$. Image from Wikipedia
   \cite{Gauss}.}
   \label{fig:gauss}
\end{figure}

In computer science, simulations are more and more used to model real
world problems, making use of todays computational power. For example, a
common way to estimate the weather of the following days is to run
multiple simulations with slightly changing parameters. Such simulations
lead to sets of correlated fields with slight differences between the
members of the ensemble. Analyzing every member individually and
comparing them to the others is infeasible, which raises the need for a
simultaneous analysis of the results. As we are extracting features from
scalar fields, we introduce the uncertain scalar field, containing the
information about the distribution of the data. Many physical phenomena
are modeled with Gaussian (normal) distributions. A Gaussian
distribution assumes that the data is distributed around a mean $\mu$
with standard deviation $\sigma$ and variance $\sigma^2$. The
probability for the value of a variable $X$ from the distribution is
given by the probability density function

\begin{equation}
  \phi_{\mu, \sigma^2}(X) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(X - \mu)^2}{2 \sigma^2}},
\end{equation}

\noindent assigning a higher probability to values closer to the mean,
leading to the characteristic Gauss bells visible in
Figure~\ref{fig:gauss}. To obtain our uncertain scalar field, we give it
the same resolution as the members of the ensemble, assuming they have
equal resolutions, and calculate the mean and the variance for every
point of the field. The mean is defined as $\mu_x = \frac{1}{m}
\sum_{k=1}^m S_k(x)$ and the variance as $\sigma_x^2 = \frac{1}{m}
\sum_{k=1}^m {(S_k(x)-\mu_x)}^2$ and therefore our uncertain scalar field
as

\begin{equation}
  S_{\mathcal{N}}(x) = \mathcal{N}(\mu_{x}, \sigma_{x}^2),
\end{equation}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Images/2DNH.pdf}
    \caption{2D neighborhood}
    \label{fig:2DNH}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Images/3DNH.pdf}
    \caption{3D neighborhood}
    \label{fig:3DNH}
  \end{subfigure}
  \caption{Support regions for the 2- and 3-dimensional case. A cell of
  this size is created for every node of the initial scalar fields,
  depending on the dimensionality of the data set. The red node marks
  the location of the actual node at this position in the scalar field,
  the green nodes belong to the cell we want to inspect and the blue
  nodes are used to calculate the gradient of the green nodes via
  central differences, as well as the gradient of the red node of
  course. The outer pink nodes are used for the gradient of the blue
  ones and with their gradient we can estimate the Hessian
  matrices of the cell nodes. The neighboring points of the blue nodes
  in Figure~\ref{fig:3DNH} are only drawn for the two upper right ones,
  as drawing all of them would make the image too cluttered.}
  \label{fig:NH}
\end{figure}

\noindent for the $m$ entities at point $x = (x_1,\dots,x_n)$. This
follows the work of Mathias Otto and Holger Theisel~\cite{Vortex},
who searched for vortex core lines in uncertain vector fields. As vector
fields have multiple values at each location, their uncertain field was
a multivariate Gaussian distribution. They used Monte-Carlo methods to
draw samples for a point and its four neighbors in 2-dimensional space
and studied the distribution of the derivatives. We will explain how
Monte-Carlo methods work in Section~\ref{sec:MGS}. Whereas the vectors
were indeed normal distributed, the derivatives were not. As the
derivatives play a vital role for the detection of vortex cores, just
like for the detection of ridges, they needed the derivatives to follow
the distribution of the sampled vectors. They solved this issue by
considering the neighboring nodes of $x$ along each dimension for
the distribution as well, to get a consistent estimation of the
respective Jacobian. For the detection of ridges in scalar fields,
we need the first and the second derivative. As we will explain in
Chapter~\ref{chap:Method}, we decide on the presence of a ridge in
a cell of the field. Therefore, we not only consider the point at
the location $x$ we are inspecting, but also the nodes of the cell
for which $x$ is the bottom left node. For all nodes of the cell we
need the first and second derivative, thus we also consider all
neighboring nodes of the cell, as well as their neighboring nodes, as we
need their derivative for the second derivative of the cell nodes. We
can take all the values from this considered neighborhood of $x$ and
interpret them as a column vector $v_x$. A lot of points in this
neighborhood influence each other, leading to an $80$-dimensional vector
in the 3D case and an $24$-dimensional vector in the 2D case. Figure
\ref{fig:NH} shows an illustration of the considered neighborhood for
one point of the fields in either case. As $v$ is multidimensional, we
now have a multivariate Gaussian distribution in our uncertain scalar
field with the underlying mean vector field $\mu(x) = \frac{1}{m}
\sum_{k=1}^m v_{x,k}$ and covariance matrix field $\Sigma(x)=
\frac{1}{m} \sum_{k=1}^m (v_{x,k} - \mu(x)){(v_{x,k} - \mu{(x)})}^\top$
and therefore

\begin{equation}
  S_{\mathcal{N}}(x) = \mathcal{N}(\mu(x), \Sigma(x)).
\end{equation}

From this distribution we will draw samples in Chapter~\ref{chap:Method}
and calculate ridge criteria on them. To generate these samples, we
need matrix decompositions of the covariance matrices, which we will
explain in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Decompositions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our Monte-Carlo method used for sampling the uncertain scalar fields
relies on the decomposition of the covariance matrix, defining the
deviations of the members from the mean. A matrix decomposition is a
factorization of a matrix into its constituent parts. In general there
are two classes of factorizations: decompositions used for solving
linear equations and decompositions based on eigenvalues and vectors. We
will consider one example from each class, namely the Cholesky and the
Eigendecomposition, as they are prominent representatives.

%----------------------------------------------------------------------%
\subsection{Cholesky Decomposition}\label{sec:cholesky}
%----------------------------------------------------------------------%

The Cholesky factorization is a decomposition of a square, hermititan,
positive definite matrix $A$ into the product of a lower-triangular
matrix L and its conjugate transpose $A = L L^*$. A hermitian matrix is
the complex counterpart to a real symmetric matrix. Two complex numbers
are complex conjugate if they have equal real and imaginary parts, but
opposite signs. For example, $1+i \in \mathbb{C}$ is the complex
conjugate of $1-i$. A simple way to determine the definiteness of a
matrix is to look at its eigenvalues. If every eigenvalue $\lambda$ of a
square $n \times n$ matrix is greater than zero, $\lambda_{i} > 0, i \in
\{1,\dots,n\}$, the matrix is positive definite. If every eigenvalue is
greater or equal to zero, $\lambda_{i} \ge 0$, the matrix is positive
semi-definite. The same goes for negative eigenvalues, as they make the
matrix negative (semi-) definite. Following this property, the standard
Cholesky decomposition is also applicable for every real valued,
symmetric, positive definite matrix, resulting in a lower triangular
real valued matrix and its transpose $A = L L^\top$. We will only deal
with real valued matrices in this work and therefore follow this
notation. An example for the Cholesky decomposition of a simple matrix
would be
\begin{equation}\label{eq:chol}
  \begin{pmatrix}
    9 & 18 & -27\\
    18 & 40 & -18 \\
    -27 & -18 & 406
  \end{pmatrix}
  =
  \begin{pmatrix}
    3 & 0 & 0\\
    6 & 2 & 0 \\
    -9 & 18 & 1
  \end{pmatrix}
  \begin{pmatrix}
    3 & 6 & -9\\
    0 & 2 & 18 \\
    0 & 0 & 1
  \end{pmatrix}.
\end{equation}

%----------------------------------------------------------------------%
\subsubsection{$LDL^T$ Decomposition}
%----------------------------------------------------------------------%

The closely related $LDL^T$ decomposition extends the classical Cholesky
decomposition to semi-definite and negative definite matrices $A = L D
L^\top$, where $L$ is a lower unitriangular matrix, meaning there are
ones on the diagonal instead of zeros, and $D$ is a diagonal matrix. The
two decompositions are related as follows:

\begin{equation}\label{eq:LDL}
  A = LDL^\top = (L D^\frac{1}{2}) (D^\frac{1}{2}  L^\top),
\end{equation}

\noindent with the matrix root of $D$. This version has the advantage
that it avoids extracting square roots, thus allowing for negative
entries in $D$, as it is possible for some indefinite matrices for which
no classical decomposition exists. The $LDL^T$ decomposition still has
the same computational complexity as the standard Cholesky
decomposition. The $LDL^T$ decomposition of the matrix from Equation
\ref{eq:chol} is

\begin{equation}
  \begin{pmatrix}
    9 & 18 & -27\\
    18 & 40 & -18 \\
    -27 & -18 & 406
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 0 & 0\\
    2 & 1 & 0 \\
    -3 & 9 & 1
  \end{pmatrix}
  \begin{pmatrix}
    9 & 0 & 0\\
    0 & 4 & 0 \\
    0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & -3\\
    0 & 1 & 9 \\
    0 & 0 & 1
  \end{pmatrix}
  .
\end{equation}

\noindent The Cholesky decomposition and its variant are a great way to
solve linear equations, as the results for the variables can easily be
seen for a triangular matrix. Also the implemented algorithms offer
fast computation with complexity $\mathcal{O}(\frac{N^3}{3})$.

%----------------------------------------------------------------------%
\subsection{Eigendecomposition}\label{sec:eigen}
%----------------------------------------------------------------------%

The Eigendecomposition is a factorization of a square $n \times n$,
diagonizable matrix $A$, into the form $A = E \Lambda E^{-1}$, where the
columns of $E$ are $n$ linearly independent eigenvectors of $A$ and
$\Lambda$ is a diagonal matrix with the respective eigenvalues. The
eigenvalue $\lambda_i$ at $\Lambda_{ii}$ corresponds to the eigenvector
$\epsilon_i$ at column $i, i \in \{1,\dots,n\}$ of $E$. This follows the
basic attribute of eigenvectors, as they only change by a scalar factor
when multiplied with their respective matrix, meaning $A \epsilon =
\lambda \epsilon$ and therefore $A E = E \Lambda$, which leads to the
decomposition $A = E \Lambda E^{-1}$. Here is a simple example of the
Eigendecomposition of a symmetric, real, diagonizable matrix:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & {-1} & 1\\
    1 & 0 & 1\\
    2 & 1 & 0
  \end{pmatrix}
  \begin{pmatrix}
    4 & 0 & 0\\
    0 & {-2} & 0\\
    0 & 0 & {-2}
  \end{pmatrix}
  \begin{pmatrix}
    0.5 & {-0.5} & 0.5\\
    {-1} & 1 & 0\\
    {-0.5} & 1.5 & {-0.5}
  \end{pmatrix}
  .
\end{equation}

\noindent An $n \times n$-dimensional matrix is diagonizable if it has
$n$ linearly independent eigenvectors. The decomposition can for example
be used to easily raise a matrix to a power, as you only have to raise
the eigenvalue matrix $\Lambda$ to that power:

\begin{equation}
  \begin{pmatrix}
    1 & {-3} & 3\\
    3 & {-5} & 3\\
    6 & {-6} & 4
  \end{pmatrix}
  ^4
  =
  E
  \Lambda^4
  E^{-1}
  =
  \begin{pmatrix}
    136 & {-120} & 120\\
    120 & {-104} & 120\\
    240 & {-240} & 256
  \end{pmatrix}
  .
\end{equation}

\noindent For the Monte-Carlo sampling, our covariance matrices don't need
to be diagonizable, as this property is only required for the inverse of
the eigenvector matrix. Section~\ref{sec:MGS} will give more information
on that matter. The Eigendecomposition plays a vital role in this work,
as the eigenvectors give a lot of information about the decomposed matrix.
The eigenvectors define major axes, like the direction of greatest change
of the gradient, or the dimension with the most uncertainty.

%----------------------------------------------------------------------%
\subsubsection{Principal Component Analysis}
%----------------------------------------------------------------------%

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\textwidth]{Images/PCA.pdf}
  \caption{Principal Component Analysis of a set of points in
  $2$-dimensional space. The green line denotes the line of greatest
  variance, whereas the blue line is orthogonal to the green one and
  goes along the least variance. These lines correspond to the
  eigenvectors of a matrix containing the $x$ and $y$ coordinates of the
  points as columns.}
  \label{fig:PCA}
\end{figure}

Even though the Principal Component Analysis (PCA) is not a decomposition
in itself, it is an application of the Eigendecomposition. In general,
the PCA fits an $n$-dimensional ellipsoid around a set of possibly
correlated points. The first principle component then corresponds to the
axis of biggest magnitude of the ellipsoid and therefore the axis of the
largest variance of the data. The principal components are always
orthogonal to each other. When the Eigendecomposition is applied to a
matrix containing correlated points, the resulting eigenvectors match
the principal components and the eigenvalues match the variance along
those axes. The eigenvalues therefore give information about the shape
of the domain. In the 2-dimensional case, if the eigenvalues are quite
similar, the data is distributed in a round circle. If the eigenvalues
differ in magnitude, the domain becomes an ellipse. See Figure
\ref{fig:PCA} for a graphical example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Box-Muller Transformation}\label{sec:BM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Box-Muller Transformation is a method to transform two uniformly
distributed random numbers into two independent, standard, normally
distributed random numbers. An example for a standard normal
distribution can be seen in Figure~\ref{fig:gauss}. Let $U_1$ and $U_2$
be samples from the uniform distribution on the interval ($0$, $1$). Let
$N_1$ and $N_2$ be:

\begin{align}
  &N_1 = \sqrt{-2 \ln{U_1}} \cos (2 \pi U_2) \\
  &N_2 = \sqrt{-2 \ln{U_1}} \sin (2 \pi U_2).
\end{align}

\noindent Then $N_1$ and $N_2$ are independent random numbers with a
standard normal distribution, meaning that their mean is $\mu = 0$ and
standard deviation $\sigma = 1$. This transformation can be repeated to
generate standard normal distributed vectors of arbitrary length. These
vectors will be used for our multivariate Gaussian sampling, as we will
scale them to fit our distributions in the uncertain scalar fields.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridges}\label{sec:Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{Images/func1D.png}
    \caption{$f(x)= -x^2$}
    \label{fig:ridge1D}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{Images/func2D.png}
    \caption{$f(x,y)= -x^2$}
    \label{fig:ridge2D}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{Images/func3D.png}
    \caption{$f(x,y,z)= -x^2$}
    \label{fig:ridge3D}
  \end{subfigure}
  \caption{Ridges of co-dimension one for the function $f(x)=-x^2$ with
  increasing dimensionality. The ridge dot (red) in~\subref{fig:ridge1D}
  is at $x=0$. Adding a dimension in \subref{fig:ridge2D} gives the
  function a line (black) for $x=0$ along $y$. The same happens if we add
  more dimensions. In \subref{fig:ridge3D} we can see the ridge surface
  lying in the volume at its maximum.}
\end{figure}

The focus of this work is the extraction of ridge features from
uncertain scalar fields. In order to do that, we first need to
understand what a ridge is in a certain setting. While the intuitive
definition of a ridge would be a path along the crest of a mountain with
terrain falling off on either side, we focus on a mathematical
definition of ridges. In principal, a ridge is a local maximum of an
$n$-dimensional function. For example, the maximum of the
$1$-dimensional function $f(x) = -x^2$ would be the $0$-dimensional
point at $x=0$, the ridge point (Figure~\ref{fig:ridge1D}). When we add
a dimension to the function, the domain becomes a $2$-dimensional
surface, and the ridge point expands into a $1$-dimensional ridge line
(Figure~\ref{fig:ridge2D}). Adding another dimension turns the function
domain into a $3$-dimensional volume and the ridge into a 2-dimensional
surface (Figure~\ref{fig:ridge3D}) and so on. In general, a ridge is a
$k$-dimensional manifold in an $n$-dimensional space for $k=\{1,
\dots, n-1\}$. In other words, ridge points and lines also exist in
$3$-dimensional domains for example. This work focuses on ridges in $2$
and $3$-dimensional domains.\\
\indent To calculate the maximum, we need to derive the function and
search for the points where the derived function is zero. The derived
function $f'(x)=-2x$ has only one point for which the function equals
zero at $x=0$. Now we only know that we have found a local extremum and
need to check whether its a maximum or minimum by looking at the second
derivative $f''(x)=-2$. As $-2 < 0$, we now know that the extremum is a
maximum. The same principal is applicable for ridges in higher
dimensions. The derivative of $f$ being zero tells us, that at this
point there is no change along the one dimension of the function. As
ridges are features of lower dimension than the scalar field, they are a
maximum for the $n-k$ dimensions, and therefore have a derivative of $0$
along those dimensions, but can still change into the directions of the
other eigenvectors. Due to the higher dimensionality and therefore
increasing complexity, ridges usually won't be aligned with the base
axes of the scalar field and the dimension along which the maximum lies
is unclear.\\
\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{Images/ridgeEV.pdf}
  \caption{Gradient (green) on a ridge line (red) in 2-dimensional space.
  The gradient is orthogonal to the blue eigenvector and parallel to the
  other eigenvector, which is aligned with the ridge. The eigenvalue
  corresponding to the blue eigenvector has to be negative.}
  \label{fig:ridgeEV}
\end{figure}
The eigenvectors of the Hessian matrix give us information about
the change of the gradient along their directions. The eigenvectors
represent the axis along which the gradient changes the most. In the
2-dimensional case, we have two eigenvectors with which we can predict
the change of the gradient in every possible direction. Now the dot
product of the gradient and the eigenvector denotes the directional
derivative. If the gradient is on an extremum like in
Figure~\ref{fig:ridgeEV}, it is orthogonal to one eigenvector, as it
only changes along the direction of the other eigenvector. For higher
dimensions, the gradient lies in a subspace spanned by the other $k$
eigenvectors which are not orthogonal to the gradient. For the extremum
to be a ridge, the eigenvalue associated with the orthogonal eigenvector
has to be negative, just like the second derivative of the 1-dimensional
function. This is due the property of the gradient that it always points
towards greatest ascend and therefore always towards the ridge. If we
now step along the direction of the eigenvector, we step straight aways
from the ridge in both ways. Therefore the gradient gets negatively
scaled against the direction taken to still point towards the ridge. The
question remaining is how to decide which eigenvectors denote the ridge
domain. Eberly~\cite{Eberly} and Lindeberg~\cite{Lindeberg} proposed two
ways on how to sort the eigenvalues $\lambda_i$ in order to pick the
right corresponding eigenvectors $\epsilon_i$:\\
\begin{inparaenum}[(a)]
  \item Eberly
  \begin{equation}\label{eq:Eberly}
   \lambda_1 \leq \cdots \leq \lambda_n
  \end{equation}
  \item Lindeberg
  \begin{equation}
    \lvert \lambda_1 \rvert \geq \cdots \geq \lvert \lambda_n \rvert.
  \end{equation}
\end{inparaenum}
\noindent Lindebergs method extracts a subset of the ridge features
found by Eberly. Lindeberg discards a feature if along any eigenvector
there is an upward bend, denoted by a positive eigenvalue, that is
stronger than the downward bend along the $n-k$ eigenvectors found by
Eberly. For the definition of Eberly, the strength of an upward bend
along the ridge is irrelevant for its existence. As a significantly
negative eigenvalue denotes a strong change along the direction of the
eigenvector, Lindeberg requires the ridges to be sharper than Eberly.
This work will build on Eberlys method and therefore the principle
definition for the existence of a $k$ dimensional ridge at point $x
\in \real^n$ is:\\

\begin{equation}\label{eq:ridgeDot}
  \nabla S(x) \cdot \epsilon_1 = \cdots = \nabla S(x) \cdot \epsilon_{n-k} = 0
\end{equation}
\begin{equation}\label{eq:ridgeEV}
  \lambda_{n-k} < 0.
\end{equation}

\noindent This is just an approximation, as ridges for example can have
nonlinear shapes and we assume linearity for the interpolation of our
data. Also we only estimate the derivatives with central differences,
giving us linear transformations. The counterpart to ridges are valleys.
These can be obtained by extracting ridges from the scalar field $-S(x)$
or by checking Equation~\ref{eq:ridgeDot} for the eigenvectors
corresponding to the $n-k$ largest eigenvalues and $\lambda_{n-k} > 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridge Extraction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{Images/MC.pdf}
  \caption{Marching Squares configuration for a ridge (red) inside a
  2-dimensional cell. The orientation of the gradients (green) to the
  minor eigenvector (blue) decides the sign assigned to the node.
  Opposite signs for two nodes connected by an edge indicate a
  possible ridge between them.}
  \label{fig:MC}
\end{figure}

While the last section talked about the formal definitions of ridges,
invariably exact results in discrete scalar fields are unrealistic.
Ridges can have nonlinear shapes and for example be curved inside a
cell. With the assumption of linear interpolation, this can cause loss
of possible ridge features. With different algorithms we can still try
to estimate the ridge as best as possible. For the certain extraction of
ridge surfaces in 3D and ridge lines in 2D, an approach using the
Marching Cubes algorithm is the usual way to approximate the ridge. For
ridge lines in 3D, the Parallel Vectors operator is used to find the
ridges. We will explain both and how we use them in our uncertain
context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertain Marching Cubes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{Images/MCTable.pdf}
  \caption{Unique triangulations of the cubes according to Lorensen and
  Cline. Image from the original Marching Cubes publication~\cite{MC}.}
  \label{fig:MCTable}
\end{figure}

With the definitions from the last section, the search for a ridge of
co-dimension one in a scalar field can be broken down to the extraction
of an isovalue, as we are searching for the points where $\nabla S(x)
\cdot \epsilon_1 = 0$. The Marching Cubes algorithm~\cite{MC} is a
method to extract isosurfaces from 3-dimensional discrete scalar fields.
It got its name due to the cell-wise extraction of the isosurface. As
the minor eigenvector is always perpendicular to a possible ridge
feature, the sign of its dot product with the gradient gives us
information about their orientation to each other. A negative sign would
mean they point in different directions, whereas a positive that they
point in similar directions. As eigenvectors yield no natural
orientation, we have to orient the eigenvectors at the nodes in the same
direction for Marching Cubes to work consistently. We can achieve this
by using the Principal Component Analysis on a matrix containing the
minor eigenvectors of the cell nodes. We then check the dot product of
the vectors with the major component and eigenvectors with a negative
one get inverted. Now we get consistent results for the dot products and
know that the gradient flipped between nodes with opposite signs.
Figure~\ref{fig:MC} shows this relation for the 2-dimensional version of
Marching Cubes, Marching Squares. With the signs for the nodes, a lookup
table is consulted, giving information on which edges have an
intersection of the isovalue. If every value is above or below the
isovalue, the cell does not contain a feature. Now linear interpolation
is used to find the location of the isosurface on the respective edges
and the found points are used asvertices of a triangle mesh.
Figure~\ref{fig:MCTable} shows the 15 unique configurations for a
surface in the cell, without rotated variants. The lookup table of the
Marching Squares version is a lot simpler, as there are only four nodes
to check. We will use the Marching Cubes approach on cells sampled from
our uncertain scalar fields to find the edges that contain a gradient
orthogonal to his minor eigenvector. As denoting the exact location of the
intersection at the edge is unimportant in the uncertain setting, we will
not generate meshes and only interpolate the gradient and the Hessian at
the intersection. Therefore, we will call it Uncertain Marching Cubes
(UMC).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Parallel Vectors Operator}\label{sec:PVO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For the extraction of ridges of co-dimension two, we cannot use the
classical Marchig Cubes algorithm, as it would essentially require to
extract two isovalues simultaneously for the orthogonality. For the
3-dimensional case, due to the symmmetry of the Hessian, being
orthogonal to two eigenvectors implies parallelity to the third.
Therefore we only need to search for the points where the gradient is
parallel to its major eigenvector, $g||\epsilon_3$. Peikert and Roth
proposed the Parallel Vectors operator~\cite{PV} as a versatile tool to
find the set of points where two vector fields are parallel. We will use
their analytic solution for triangular faces, and therefore every face
of a cell is split into two triangles. The vectors of the gradient field
and their derivatives at the nodes of an examined triangle can be
expressed as a function of local triangle coordinates $s$, $t$ and $u$
with $u$ set to $1$:
\begin{equation}
  g = V
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}
\end{equation}
\begin{equation}
  \epsilon_3 = W
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}.
\end{equation}
\noindent $V$ and $W$ are $3 \times 3$ matrices that are constructed
from the vectors $v_i$ and $w_i$ at the three nodes of the fields with:
\begin{align}
  &v_{12} = v_2 - v_1\\
  &v_{13} = v_3 - v_1\\
  &V = (v_1, v_{12}, v_{13})
\end{align}
\noindent and respectively for $w_i$ and $W$. Now two fields are
parallel when:

\begin{equation}
  V
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}
  = \bar{\lambda} W
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}
\end{equation}

\noindent If either $V$ or $W$ is invertible, ergo $\det{(V)}\neq 0 \vee
\det{(W)}\neq 0$ we can adjust the equation to get an eigenvector
problem:

\begin{equation}
  W^{-1} V
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}
  = \bar{\lambda}
  \begin{pmatrix}
    s\\
    t\\
    1
  \end{pmatrix}
\end{equation}

\noindent Usually the matrix with the larger determinant is being
inverted. Now we can calculate the eigenvectors for the matrix $W^{-1}
V$ and scale them such that their last component is $1$. This provides
us with the local triangle coordinates $s$ and $t$ where the two fields
are parallel. If $s+t \leq 1$ and $s$ and $t$ are both greather than
zero, the point lies within the triangle and an extremum is found. With
the triangle coordinates, the Hessian at the point can be interpolated to
check for $\lambda_2 < 0$. The same is applicable for valleys with
parallelity of the gradient to the minor eigenvector and $\lambda_2 > 0$.
At the end, the found points for a certain case are connected to form a
line.
