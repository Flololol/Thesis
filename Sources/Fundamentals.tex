%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Fundamentals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter provides a basic understanding of the underlying methods
and concepts, that were required for this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Grids}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In computational science, discrete data domains are often represented by
grids. The data itself is mostly saved at specific points of the grid
(nodes), or in regions (cells), enclosed by surrounding nodes and the
respective connections (edges) between them. In more rare cases the
values are saved in the edges or the faces of the cells. The
connectivity of the nodes is given by the topology of the grid and
therefore the shape of the cells. There are three types of grids:
scattered data, which has no topology, hence no edges connecting the
nodes, structured grids, which have an implicit topology following the
ordering of the nodes, with a fixed number of nodes per dimension, as
well as fixed cell types, and unstructured grids, which only have
irregular topology with varying cell types. For the latter the topology
has to be stored explicitly. Structured grids can further be
distinguished into uniform, rectilinear and curvilinear (irregular)
structured grids. The nodes in uniform grids are equidistant for every
dimension, whereas rectilinear grids may have irregular spacings along
either axis and curvilinear grids may have irregular spacings between
each grid node. This work focuses on uniform structured grids as it
makes it easier to compare multiple grids at specific points in the
domain.\\
INSERT PICTURE TO DIFFERENT GRID TYPES

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional field with a single scalar value at every point in
space is called a scalar field. A simple example for a scalar field is a
height map of some geographical terrain. It has two dimensions with a
height value at every point encoded with color. In this work we will use
the notation $S(x)$ for the scalar value at point $x = (x_1,\dots,x_n)$
in the scalar field $S$ with $S: \real^n \rightarrow \real$. While in
continuous scalar fields the values of every point are defined by a
function, discrete scalar fields only have values at specific points in
space. As real world data is often acquired by measuring certain
locations and usually does not follow any graspable function, we will
focus on discrete scalar fields in this work. This also applies to the
other types of fields we will encounter, because they have the same 
resolution as the scalar field.\\
INSERT PICTURE OF HEIGHT MAP

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertain Scalar Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Notation of uncertainty. Read Paper!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vector Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional vector field $V$ associates an $m$-dimensional vector
with every point in space $V(x_1,\dots,x_n):\real^n \rightarrow \real^m$.
If the vector field is the result of deriving a scalar field, thus the 
vector field is the gradient of the scalar field, the vector field is
called conservative. Conservative vector fields have the property, that
the line integral along any path connecting two points is always equal.
Since we are extracting features from scalar fields in this work, our
vector fields will always be conservative and $n = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensor Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An $n$-dimensional tensor field $H$ associates an $l \times
m$-dimensional tensor with every point in space $H(x_1,\dots,x_n):
\real^n \rightarrow \real^{l \times m}$. Tensor fields are a
generalization of scalar and vector fields as a tensor with $l = m = 1$
would represent a scalar, and with $l > 1$ and $m = 1$ a vector. In our
case, the tensor field is a result of deriving a vector field, thus $n =
l = m$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since we are dealing with discrete scalar fields, we have no function we
could derive to get the underlying gradient field. Instead we have to
approximate the derivatives with finite difference methods. There are
three forms which are commonly used:\\
\\
\begin{inparaenum}[(a)]
  \item Forward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_i)}{h}
  \end{equation}
  \item Backward Differences
  \begin{equation}
    \nabla f(x_i) = \frac{f(x_i) - f(x_{i-1})}{h}
  \end{equation}
  \item Central Differences
  \begin{equation} \label{eq:centDiff}
    \nabla f(x_i) = \frac{f(x_{i+1}) - f(x_{i-1})}{2h}
  \end{equation}
\end{inparaenum}
where $f(x_i)$ denotes the $i$-th point of the function $f(x): \real \rightarrow \real$
and $h$ the distance between two neighboring points. Forward and backward
differences are used at the borders of the field, where no previous or
succeeding point is available. As we will explain in Chapter~\ref{chap:Method},
with our current implementation we process cells with a fixed size and
therefore do not consider border cases, thus this work focuses on central
differences.

%----------------------------------------------------------------------%
\subsection{First Derivative}
%----------------------------------------------------------------------%

When deriving an $n$-dimensional scalar field $S$, we need to apply
Equation~\ref{eq:centDiff} for every dimension. This results in $n$
scalar values, representing the change of the scalar field in the
respective dimensions. These values can be interpreted as an
$n$-dimensional vector, pointing in the direction of greatest ascend for
every point of the scalar field. This is the gradient field $\nabla
S(x)$.

%----------------------------------------------------------------------%
\subsection{Second Derivative}
%----------------------------------------------------------------------%

The second derivative of an $n$-dimensional scalar field is an $n$
-dimensional tensor field with $\nabla^2 S:\real^n \rightarrow \real^{n \times n}$.
This tensor field can again be obtained with the central difference
method applied to every dimension of the gradient field. Here we get an
$n$-dimensional vector per dimension as we are subtracting the previous
gradient from the succeeding gradient of point $x$ along any dimension.
These vectors represent the respective columns of the so called Hessian
matrix. The Hessian matrix $H$ is a specification of the Jacobian Matrix $J$,
which derives any vector valued function $f(x): \real^n \rightarrow \real^m$,
$J \in \real^{m \times n}$. The Hessian matrix is obtained when deriving
a conservative vector field, therefore the Hessian is quadratic with:

\begin{equation}
  H =
  \begin{bmatrix}
    \frac{\partial^2 S}{\partial x_1^2} & \frac{\partial^2 S}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_1 \partial x_n}\\
    \frac{\partial^2 S}{\partial x_2 \partial x_1} & \frac{\partial^2 S}{\partial x_2^2} & \dots & \frac{\partial^2 S}{\partial x_2 \partial x_n}\\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 S}{\partial x_n \partial x_1} & \frac{\partial^2 S}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 S}{\partial x_n^2}
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}

\noindent According to Schwarz's theorem of the symmetry of second
derivatives ([ref]), the Hessian is assumed to be symmetric, therefore
the eigenvectors are orthogonal and the eigenvalues are real valued.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Interpolation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our discrete fields only provide information for a limited number of
points along a dimension, therefore we need to estimate the values lying
between the known locations. To ease computation we assume that the
underlying functions of the fields are linear and apply linear
interpolation to the tensors $A, B \in \real^{n \times n}$ at the
neighboring points, independent of dimensionality. As we will explain in
Chapter~\ref{chap:Method}, we only apply interpolation on the edges of
cells and can therefore assume the distance between the two nodes to be
$1$. This leads to the interpolated tensor:

\begin{equation}
  T_{ipol} =
  \begin{bmatrix}
    (1-t) \cdot A_{11} + t \cdot B_{11} & \dots & (1-t) \cdot A_{1n} + t \cdot B_{1n} \\
    \vdots & \ddots & \vdots \\
    (1-t) \cdot A_{n1} + t \cdot B_{n1} & \dots & (1-t) \cdot A_{nn} + t \cdot B_{nn} \\
  \end{bmatrix}
  \in \real^{n \times n}
\end{equation}
with $t$ being the relative distance of the desired location to tensor
$A$. (see Figure ipol) We will only consider the cases for $n \in
\{1,2,3\}$. \\
(Picture explaining linear interpolation)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Decompositions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A matrix decomposition is a factorization of a matrix into its
constituent parts. In general there are two classes of factorizations:
decompositions used for solving linear equations and decompositions
based on eigenvalues and vectors. We will consider one example from each
class, namely the Cholesky and the Eigendecomposition. As we will
explain in Section~\ref{sec:MGS} we will use both decompositions to
generate samples from a multivariate gaussian distribution, even though
they are from different classes.

%----------------------------------------------------------------------%
\subsection{Cholesky Decomposition}
%----------------------------------------------------------------------%



%----------------------------------------------------------------------%
\subsection{Eigendecomposition}
%----------------------------------------------------------------------%

%----------------------------------------------------------------------%
\subsubsection{Principal Component Analysis}
%----------------------------------------------------------------------%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multivariate Gaussian Sampling} \label{sec:MGS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Marching Ridges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Vectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
