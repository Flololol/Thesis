%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Method}\label{chap:Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter explains our principle approach on how to extract ridge
features from uncertain scalar fields. In the usual case, ridges do not
lie directly on the nodes of a cell, but inbetween. Therefore it is
insufficient to only look at single nodes of every member of the set of
possibly correlated scalar fields to decide on the presence of a ridge.
Thus in the $3$-dimensional case the actual node at the location we
examine is the bottom left node of a cell of 8 nodes (see red node in
Figure~\ref{fig:3DNH}). This follows an implementation detail, as our
scalar fields are traversed from bottom left to top right. Sampling
these 8 nodes with a Monte-Carlo method would only give us Gaussian
distributed values over the range of values at these locations, but no
information about their change in either dimension outside the cell.
Otto and Theisel~\cite{Vortex} tested in their work to derive a velocity
vector $v$ in a two dimensional uncertain vector field by sampling the
vector and its four neighboring vectors individually from their
distributions. They then estimated the Jacobian $J$ with central
differences on the neighboring vectors and multiplied it with the
velocity vector, to get the acceleration vector $a = J v$. The resulting
vectors were not Gaussian distributed anymore. They solved this issue by
instead of sampling each vector individually, they sample the whole
neighborhood of the vector to keep track of its change along the
dimensions. Therefore we also inspect the 24 nodes adjacent to the 8
nodes of the cell, to be able to calculate the gradient via central
differences. As Theisel \etal{} were searching for vortex core lines in
uncertain vector fields, they only needed to derive their field once, to
be able to decide on the presence of a core line based on the
eigenspace. As we base our detections on eigenvalues and vectors as
well, we need to derive our uncertain scalar field twice. We therefore
also add the adjacent nodes of the 24 supporting nodes to our considered
neighborhood. This leads to 80 nodes making up the cell we want to draw
samples from (see Figure~\ref{fig:NH}). With this we can create the
uncertain scalar field from Section~\ref{sec:USF}. It is important to
remember that we cannot compute ridge lines and surfaces directly on the
uncertain scalar field, but on samples from it. Thus we sample the field
multiple times and the result is a probability for the existence of the
desired ridge feature in the cell.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multivariate Gaussian Sampling}\label{sec:MGS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our uncertain scalar field $S_{\mathcal{N}}$ at point
$x=(x_1,\dots,x_n)$ consists of a multivariate Gaussian distribution,
made up from the mean vector $\mu_x$ and the covariance matrix
$\Sigma_x$. The mean vector portrays the average values of the
neighborhood of $x$ while the covariance matrix contains the variances
of each value to each other around this mean. Unfortunately we cannot
compute ridge criteria from these objects. Monte-Carlo methods offer a
way to draw samples from multivariate normal distributions. The basis of
multivariate Gaussian sampling is the affine transformation property of
normal distributed vectors that states that any linear transformation of
a normal vector is again normal:

\begin{equation}
    X \sim \mathcal{N}(\mu,\Sigma) \Rightarrow AX \sim \mathcal{N}(A\mu, A\Sigma A)
\end{equation}

\noindent for any dimensionality $d$ of vector $X$ and $d \times d$
matrix $\Sigma$ and any $k \times d$ matrix $A$. By this property we
know that if we have a standard normal distributed vector $Z \sim
\mathcal{N}(0,I)$ with $I$ being an $d \times d$ identity matrix, then
$X = AZ + \mu$ is from the distribution $X \sim \mathcal{N}(\mu, A
A^\top)$. Now we simply have to find a matrix $A$ for which $AA^\top =
\Sigma$. The Cholesky (Section~\ref{sec:cholesky}) and the
Eigendecomposition (Section~\ref{sec:eigen}) are the two most popular
methods to calculate such a matrix as the Cholesky decomposition is
computationally efficient and the Eigendecomposition deliveres more
numerical stability as well as a lot of predefined librarys. As our
covariance matrices are only positive semi-definite, we need the
extended version of Cholesky. Now we get the two decompositions:
\begin{align}
    &\Sigma_x = L D L^\top \\
    &\Sigma_x = E \Lambda E^{-1},
\end{align}
In theory, a suitable matrix $A$ would now be either $A=L
D^{\frac{1}{2}}$ or $A=E\Lambda^{\frac{1}{2}}$ for $\Sigma = AA^\top$.
In practice, because of our relatively huge $80 \times 80$ dimensional
covariance matrix we get negative elements in the diagonal matrix $D$
even though our covariance matrix is positive semi-definite. Therefore
we cannot take the root of $D$. The same issue goes for the
Eigendecomposition where the eigenvalues become infinitesimally small
negative values, hindering the computation of the root of $\Lambda$. As
the sign of the eigenvalues determines the definiteness of a matrix,
this makes our covariance matrix technically indefinite, as we have
positive and negative eigenvalues. We solved this issue by simply
multiplying the components without taking the root of the diagonal
matrix, hence $A = LD$ and $A = E \Lambda$. This did not seem to affect
the samples drawn with the Eigendecomposition, but the samples generated
with Cholesky showed some inconsistencies. Section (INSERT REF) will
give more insight on that topic and compare the decompositions. Now we
can proceed with the found $A$. In the 3 (2) dimensional  case we
generate an 80 (24) dimensional standard uniformly distributed vector
and transform it into a standard normal distributed vector $Z$ using the
Box-Muller transformation (Section~\ref{sec:BM}). Now we multiply $A$
with $Z$ and add the mean vector of this location to get the sample
vector $X = AZ + \mu_x, X \sim \mathcal{N}(\mu_x, \Sigma_x)$. This is
only a single instance from the multivariate distribution, thus we
repeat the sampling multiple times per cell to try to model the
distribution as best as possible. With the now obtained sample vectors
we can start to calculate ridge criteria at the location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ridge Extraction}\label{sec:ridgeextract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we will cover the extraction of a ridgesurface in a
3-dimensional scalar field. The 80-dimensional sample vector is split up
according to the original nodes and then the 32 gradients of the first
derivative are calculated via central differences. From them we estimate
the 8 Hessians of the cell gradients. The conservative approach to
extract the ridge surface would now be to calculate the dot product of
the gradient and the minor eigenvector at every node and apply the
modified Marching Cubes algorithm for isovalue 0, but this faces us with
a problem that occurs when generating samples from a multivariate
Gaussian distribution. Depending on the uncertainty at the location, the
distribution of the values of the sample vector might vary heavily.
Figure~\ref{fig:sampComp} illustrates this problem with samples drawn
from a distribution in 2-dimensional space. Figure~\ref{fig:gradient}
shows the scalar field for the function $f(x,y) = x^2$ for $x, y \in
\{0,\dots,5\}$ and the gradients for the nodes of the cell in the
middle. As you can see, the gradients are parallel and increase along
the x-axis. We rotated that field by 90 degrees three times, so that the
gradient in each field points orthogonal to the previous. Now we build
the Gaussian distribution for these fields and draw five samples using
the Eigendecomposition. We plot the gradients of the sampled fields, as
they make the understanding of the samples easier.
Figure~\ref{fig:samples} shows the gradients of each sample together
with their eigenvectors crossing at the respective nodes. The individual
samples relate to the parallel orientation of the original fields, but
their general direction is mixed up between the axes and they vary in
magnitude. The gradients of the green sample slightly point towards each
other, indicating that the field is increasing between the nodes, which
is never the case originally. Also interesting to note is that the
orientation of the eigenvectors is identical for every sample, only the
corresponding eigenvalue changes. Such heavy changes of the direction of
the gradient inbetween members are possible, as the gradient always
points towards the ridge. If the gradient is close to it, small changes
in the position of the ridge can result in big directional changes. This
makes the computation of ridge surfaces via Marching Cubes quite
inconsistent, even without noise on the data.
\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/gradient.png}
        \caption{}
        \label{fig:gradient}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/samples.png}
        \caption{}
        \label{fig:samples}
    \end{subfigure}
    \caption{(a) Gradients in the field $f(x,y)=x^2$. The major
    eigenvector (red) is parallel to the gradient as the gradient only
    changes along the x-axis. The minor eigenvector (blue) is orthogonal
    to them with eigenvalue 0. (b) Gradients of samples from the
    Gaussian distribution over four rotated variants of the field from
    Figure~\ref{fig:gradient} with their common eigenvectors. The
    eigenvectors are displayed as lines, as they yield no natural
    orientation. These samples were obtained with the
    Eigendecomposition.}
    \label{fig:sampComp}
\end{figure}
If we would now try to extract ridges with the conventional
Marching Cubes algorithm and only accept features where the dot product
of the gradient and the minor eigenvector is exactly 0, we would find
nothing. This is especially true for interpolated vectors as a whole,
due to numerical issues. We can try to challenge this problem by giving
a tolerance $t$ for the dot product to be 0, but the optimal value for the
tolerance depends on the uncertainty at the location and basically the
knowledge of where the ridge actually is. To further illustrate this
problem, we took the 3-dimensional scalar field corresponding to the
function $f(x,y,z) = \cos{(x)} + \cos{(y)} + \cos{(z)}$ in the domain
$[0, 4 \pi] \times [0, 4\pi] \times [0, 4\pi]$. This function is
periodical and therefore we can easily shift the whole domain along a
dimension by adding constants to $x, y$ or $z$. Figure~\ref{fig:ridge}
shows the certain ridge surface for this scalar field. As we have a
cosinus function starting at 0 and ending at $4\pi$ for every dimension,
we have two full waves along either dimension with their maxima in the
middle at $2\pi$. This can be easily seen in the Figure with small
surfaces orthogonal to the $x$ and $y$ dimension and a bigger surface
seperating the two waves in $z$ direction. The ridge surfaces below the
bigger surface are identical to the ones visible above. Now we add small
constants drawn from a uniform distribution to $x, y$ and $z$ to
generate an ensemble of scalar fields with their ridge moved around the
original one. Now we calculate the uncertain ridge surface of the
distribution with 100 samples drawn per cell and only require the
absolute of the dot product to be smaller than $t=0.05$. The resulting
surface is shown in Figure~\ref{fig:MCridge}. Holes are visible at
equivalent distances. They are at the mutual inflection points of the
scalar field. Here, the values of the scalar field are close to zero and
therefore the ordering of the eigenvalus by Eberly (Equation
\ref{eq:Eberly}) is inconsistent, as the eigenvalues are very similar.
If we now increase the tolerance to try to fill these holes, we pick up
a lot of undesired features. For $t=0.25$ (Figure~\ref{fig:MCridgetol})
the holes are still visible, but already a lot of undesired features
clutter the view. For $t=0.5$ (Figure~\ref{fig:MCridgetop}) the holes are
barely visible in the broad noise on the surface, but the smaller surfaces
on top and bottom are hard to distinguish from the surrounding false
positives. This brings in the need for an approximate ridge criterion
that adapts to local properties of the field.

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/sfield.png}
        \caption{field}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/ridge.png}
        \caption{surface}
    \end{subfigure}
    \caption{Ridge surface of the scalar field $f(x,y,z)=\cos(x)+\cos(y)
    +\cos(z)$ with $x,y,z \in (0, 4\pi)$. The functions are periodic and
    therefore the ridges are symmetric. The z axis is squished, even though
    it has the same data range as the other axes to ease the spatial
    understanding of the domain.}
    \label{fig:ridge}
\end{figure}
\begin{figure}
    \centering
    \begin{subfigure}[]{0.7\textwidth}
        \includegraphics[width=\textwidth]{Images/MCridge.png}
        \caption{$t=0.1$}
        \label{fig:MCridge}
    \end{subfigure}
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/MCridgetol.png}
        \caption{$t=0.25$}
        \label{fig:MCridgetol}
    \end{subfigure}
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/MCridgetoltop.png}
        \caption{$t=0.5$}
        \label{fig:MCridgetop}
    \end{subfigure}
    \caption{Uncertain ridge surfaces from the Marching Cubes algorithm
    for different tolerances for the dot product of the gradient and the
    minor eigenvector. For $t=0.1$ the big surface along $z$ has holes
    at the inflection points of the field, where the values are close to
    zero and some undesired features at the edges of the upper and lower
    surfaces. As we increase the tolerance, the holes become smaller,
    but a lot more false positives occur. Figure (c) offers a top down
    view on the domain as there is a lot of cluttering on the sides.}
    \label{fig:MCridges}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Estimated Ridge Extraction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The underlying concept of the multivariate Gaussian sampling is to
approximate the distribution by drawing enough samples from it, that
the variety of the set is covered by the different samples. This
suggests that it might be a good idea to consider ridge criteria that
deliver an approximate result of the existence of a ridge in a small
area.
