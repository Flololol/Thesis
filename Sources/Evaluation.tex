%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}\label{chap:Eval}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter will evaluate the results obtained with our new criterion,
compare them with the conventional method, as well as show its vantages
and disadvantages. But at first, we will talk about the differences of
the matrix decompositions used to generate the samples from the
multivariate distributions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparison of Matrix Decompositions}\label{chap:evalMD}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Multivariate Gaussian sampling is a common way to model distributions
by drawing variants from the set. This method makes use of the
factorization of the covariance matrix $\Sigma$ from the distribution into
the product of two matrices, such that $\Sigma = AA^\top$. Literature
like ``Random Number Generation and Monte Carlo Methods''~\cite{Monte}
state, that any decomposition that achieves such a factorization is
suitable for the genration of random samples and delivers equally
good results. Therefore one could prefer the Cholesky decomposition
because of its faster computation time $\mathcal{O}(N^3/3)$ over
the Eigendecomposition with $\mathcal{O}(N^3)$. To compare the two
decompositions, we take the function from Figure~\ref{fig:sfield} and
equally shift it along the $x$-dimension to create a set of twenty
members with a high uncertainty along $x$, but no uncertainty along
the other dimensions, ridgewise. Figure~\ref{fig:decomps} shows the
result of the uncertain ridge calculation for both decompositions using
the new criterion. The surfaces exhibit heavy differences. While the
Eigendecomposition deliveres a smooth estimation with a constant high
probability for the ridges that every field contains and a blue cloud
with low probability where the ridge is uncertain, Cholesky has a
distorted ridge perpendicular to $y$ as well as three to four distinct
ridge structures across $x$ with a low probability. The result of the
Cholesky decomposition is considerably worse.

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/shiftXeigen.png}
        \caption{Eigendecomposition}
        \label{fig:shiftXeigen}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/shiftXcholesky.png}
        \caption{Cholesky}
        \label{fig:shiftXcholesky}
    \end{subfigure}
    \caption{Comparison of the two matrix decompositions for a set
    of fields shifted equally along the $x$ dimension. The 
    Eigendecomposition gives a smooth estimate of the ridge surface,
    according to the distribution of the ridges in the members.
    Cholesky exhibits lower probabilities at points where the
    ridge is very certain.}
    \label{fig:decomps}
\end{figure}

\noindent To understand what happened here, we will again take a look at
samples drawn from the distribution of Figure~\ref{fig:sampComp}.
Figure~\ref{fig:MDsampComp} shows the gradients of two samples from
either decomposition with their respective unscaled eigenvectors at the
base. Even though the Eigendecomposition in Figure~\ref{fig:sampleEig}
breaks the parallelity of the original field, the gradients still point
in the same general direction and the eigenvectors are consistently
oriented. The gradients of Cholesky on the other hand have no
directional correlation at all. The direction of the vectors is
arbitrary along the distribution and therefore their eigenvectors
have inconsistent orienting as well. The Eigendecomposition seems
to keep the correlation of elements of individual members, whereas
Cholesky mingles the members in one sample. For the calculation of
a ridge feature which depends on precision, this is an issue. The
differences are the consequence of the immanent nature of the
decompositions. The Eigendecomposition of the covariance matrix
equals to the Principal Component Analysis and therefore the scaled
eigenvectors equal to the uncertainty of $\Sigma$ along
the direction of the eigenvector. If we now multiply a standard normal
distributed vector $N$ with $A = E\Lambda$, every $i$-th component of
$N$ is multiplied with a component of the $i$-th column of $A$, and thus
the $i$-th eigenvector. This leads to a constant scaling of elements
along the direction of the eigenvectors. Cholesky produces a lower
triangular matrix for $A=LD^{\frac{1}{2}}$, thus the columns have
increasing influence on the distribution of the sample with increasing
$i$, but the first element of the sample vector is only determined by
the scaling of $a_{11}$ with the random number at $N_1$. An interesting
implementation detail is, that the algorithm calculating the eigenvalues
for our large covariance matrices uses the power iteration method, that
estimates the largest absolute eigenvalue $\lambda$ in $\mathcal{O}
(N^2)$. Then the matrix is reduced by $A-\lambda I$ and power iteration
is applied again. This process can be repeated until all eigenvalues are
found. Analytic results are too computationally intensive for matrices
of size $80 \times 80$ or $24 \times 24$. Due to this algorithm, the
first column of our eigenvectormatrix always corresponds to the largest
eigenvalue, and therefore the axis with the greatest variance of the
set. Comparing this to the Cholesky decomposition, we could say that the
samples from Cholesky are mainly influenced by one axis of variance of
the data set, and decreasingly influenced by the latter variances,
resulting in more random samples. Ond\v{r}ej Straka \etal{\cite{MD}}
studied the different matrix decompositions in the context of Unscented
Kalman Filters, but their results are applicable for our case as well.
They drew three conclusions after their analysis of the decompositions:
If the variable we want to sample contains no correlated elements, the
samples for both decompositions are equally good and therefore the
Cholesky decomposition might be prefered because of its faster
computation time. If the variable contains correlated elements, the
choice may significantly affect the quality of the sample and if the
elements exhibit strong correlation, numerical stability becomes an
issue as well, especially for Cholesky. Considering, that we usually
apply the uncertain ridge extraction to data sets obtained from
simulations, we will have strong correlation of the members of the data
set for the most parts of the field. We will talk about the numerical
stability in Chapter~\ref{chap:Discu}. The difference of the
decompositions is particularly visible when comparing the results
obtained from the ridge extraction using Marching Cubes
(Figure~\ref{fig:MCcomp}). Cholesky produces the same structure as in
Figure~\ref{fig:shiftXcholesky}, but with more certainty. This is a
misleading representation of the underlying uncertainty. The results
obtained from the Eigendecomposition (Figure~\ref{fig:MCeigen}) on the
other hand can almost be considered better than the ones with our new
criterion, as the high probabilities are notably thinner distributed,
just like the certain ridge would be. We will compare the two approaches
in the next section. Concluding from all of this, the Eigendecomposition
delivers better results in most cases. Only if we have strong uncertainty
in the data, the Cholesky decomposition may give us better results.
We took the set from Figure~\ref{fig:MCridges} and extended it with
members having greater variance along either axis. The results of the
two decompositions can be seen in Figure~\ref{fig:HUCcomp}. Here
Cholesky gives us higher probabilities for nodes close to the mean of
the ridges. Due to the high variance of the data set, the members seem
to lose their correlation. The downside is, that this case is not the
usual thing for real world data, where we try to extract ridge structures
from similar scalar fields. 

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/sampleEig.png}
        \caption{Eigendecomposition}
        \label{fig:sampleEig}
    \end{subfigure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/sampleChol.png}
        \caption{Cholesky}
        \label{fig:sampleChol}
    \end{subfigure}
    \caption{Gradients of samples from the distribution of the rotated
    field for $f(x,y)=x^2$. The samples from the Eigendecomposition
    keep the structure of the original fields, whereas the Cholesky
    decomposition mixes the distributions in one sample.}
    \label{fig:MDsampComp}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/shiftXold.png}
        \caption{Eigendecomposition}
        \label{fig:MCeigen}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/shiftXoldchol.png}
        \caption{Cholesky}
        \label{fig:MCchol}
    \end{subfigure}
    \caption{Comparison of matrix decompositions for the uncertain
    ridge extraction using the Marching Cubes algorithm.}
    \label{fig:MCcomp}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/highuncEigen.png}
        \caption{Eigendecomposition}
        \label{fig:HUCeigen}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/highuncChol.png}
        \caption{Cholesky}
        \label{fig:HUCchol}
    \end{subfigure}
    \caption{Comparison of matrix decompositions for a set with high
    variance in every dimension. The Eigendecomposition exhibits a ridge
    structure appearing roughly around the mean, hardly distinguishable 
    from the noise surrounding it. Cholesky delivers a more certain
    structure, with holes in the cross like ridges on top and bottom,
    due to the up and downward distortion of the members.}
    \label{fig:HUCcomp}
\end{figure}